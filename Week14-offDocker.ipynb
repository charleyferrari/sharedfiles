{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import json\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as f:\n",
    "     modeldata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another notebook (ran on the machine learning docker) I included a few runs of the emotion detection neural net, and brought in my results here for analysis.\n",
    "\n",
    "In order to test how local minima and/or saddle points might be affecting my accuracy, I decided to test out a few optimizer algorithms, on the same model, provided with Keras. I would then plot my training and test accuracy over time and see what sort of patterns I noticed.\n",
    "\n",
    "I'm using [this post](http://www.turingfinance.com/misconceptions-about-neural-networks/#algo) for inspiration on how to pick the optimizers I'm testing. It shows how the Keras optimizers correspond to a saddle point in a function.\n",
    "\n",
    "I picked the Keras default: the RMSProp algorithm, along with the Nesterov Adam and the Adadelta algorithms. The Nesterov Adam optimizer seems to be a momentum based strategy, which seem to either get permanently stuck in the saddle point, or spend a few more cycles trying to eventually get out of it down a gradient in another dimension.\n",
    "\n",
    "The Adadelta algorithm, on the other hand, didn't even hit the saddle point, and rather curved in the direction of the other minimum right away.\n",
    "\n",
    "Image recognition is highly dimensional, so I won't expect to meaningfully see these saddle points and/or local minima in this way. However, by looking at the training and test accuracy over time, I should be able to see \"flat\" periods and interpret them as getting caught in one of these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~charleyferrari/1132.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    go.Scatter(\n",
    "        x = np.arange(0,50),\n",
    "        y = modeldata['model']['history']['acc'],\n",
    "        name = 'RMSProp Training Accuracy'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x = np.arange(0,50),\n",
    "        y = modeldata['model']['history']['val_acc'],\n",
    "        name = 'RMSProp Test Accuracy'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x = np.arange(0,50),\n",
    "        y = modeldata['model2']['history']['acc'],\n",
    "        name = 'Adadelta Training Accuracy'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x = np.arange(0,50),\n",
    "        y = modeldata['model2']['history']['val_acc'],\n",
    "        name = 'Adadelta Test Accuracy'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x = np.arange(0,50),\n",
    "        y = modeldata['model3']['history']['acc'],\n",
    "        name = 'Nesterov Adam Optimizer Training Accuracy'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x = np.arange(0,50),\n",
    "        y = modeldata['model3']['history']['val_acc'],\n",
    "        name = 'Nesterov Adam Optimizer Test Accuracy'\n",
    "    )\n",
    "    \n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    updatemenus = [\n",
    "        dict(\n",
    "            x = -0.10,\n",
    "            y = 1,\n",
    "            yanchor = 'top',\n",
    "            buttons = [\n",
    "                dict(\n",
    "                    args = ['visible', [True, True, False, False, False, False]],\n",
    "                    label = 'RMSProp',\n",
    "                    method = 'restyle'\n",
    "                ),\n",
    "                dict(\n",
    "                    args = ['visible', [False, False, True, True, False, False]],\n",
    "                    label = 'Adadelta',\n",
    "                    method = 'restyle'\n",
    "                ),\n",
    "                dict(\n",
    "                    args = ['visible', [False, False, False, False, True, True]],\n",
    "                    label = 'Nesterov Adam Optimizer',\n",
    "                    method = 'restyle'\n",
    "                ),\n",
    "                dict(\n",
    "                    args = ['visible', [True, True, True, True, True, True]],\n",
    "                    label = 'All',\n",
    "                    method = 'restyle'\n",
    "                )\n",
    "            ],\n",
    "            active = 3\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename = 'week14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does indeed show some interesting behavior. There appears to be a local minima or saddle point that is giving me an accuracy ~0.88. Interestingly, the RMSProp algorithm seems to be the quickest to escape this point, very closely followed by Adadelta. The Nesterov Adam Optimizer however doesn't appear to be able to escape this point.\n",
    "\n",
    "My intuition surrounding these sorts of optimizers (partially using the blog post above, as well as [this](http://sebastianruder.com/optimizing-gradient-descent/) description of algorithms)involves around trying to imagine saddle points in higher dimensions. In three dimensions, we think of a saddle point as a point which is a minima in one dimension and a maxima in another. Optimizers partially work by finding regions with gradients of 0. Because we're \"descending\", it's difficult to end up at a local maxima, but it is possible to descend into a saddle point. Once there, a gradient descent algorithm might be fooled into thinking it's a minimum, rather than finding the dimension where it can still descend.\n",
    "\n",
    "Extending this thinking to higher dimensions, one could imagine points with a 0 derivative, and several gradients varying between positive and negative. If we have an 11 dimensional hyperplane for example, one could imagine a saddle point where it is a local minima in 9 dimensions and a local maxima in the 10th dmension.\n",
    "\n",
    "We can't be sure about what exactly this point looks like, but it does seem that the Nesterov Adam Optimizer is being fooled by a point that is not fooling the RMSProp or Adadelta algorithm.\n",
    "\n",
    "Below I'll include the code I used to create this model. I reused my image collection pipeline, and the only real change I made was defining the model callback, and exporting my results to json for analysis here (something to protect my sanity given the amount of time these models take to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(150, 150, 3), dim_ordering='tf'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, dim_ordering='tf'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, dim_ordering='tf'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below in the compile step is where I would define my algorithm (here it's RMSProp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())  \n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/root/sharedfolder/facial_expressions/Test2/Train/',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '/root/sharedfolder/facial_expressions/Test2/Validation/',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=1000,\n",
    "        nb_epoch=50,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once my history is defined, I would add it to my model json below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelData = dict()\n",
    "\n",
    "modelData['model1'] = dict(loss = 'binary_crossentropy', \n",
    "                          optimizer = 'rmsprop', \n",
    "                          history = history.history, \n",
    "                          config = history.model.get_config())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
